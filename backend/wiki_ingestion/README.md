# GitHub Wiki Ingestion System

A robust, SOLID-architecture-based system for crawling GitHub wikis, extracting content, following links, and preparing data for Milvus vector database embedding.

## ğŸ¯ Features

âœ… **Complete Wiki Crawling**
- Crawls entire GitHub wiki starting from main URL
- Configurable depth and page limits
- BFS (Breadth-First Search) crawling strategy
- Intelligent URL normalization and deduplication

âœ… **Private Repository Support**
- Git-based cloning for private wikis
- GitHub token authentication
- Access to enterprise/private repositories
- Direct markdown file processing

âœ… **Content Extraction**
- Extracts clean content from HTML
- Converts to markdown format
- Preserves document structure
- Extracts metadata (title, description, etc.)

âœ… **Linked Content Processing**
- Identifies all URLs within wiki pages
- Fetches and processes linked content
- Distinguishes internal vs external links
- Configurable or unlimited URL processing

âœ… **Smart Chunking**
- Respects markdown structure (headers, paragraphs)
- Configurable chunk size (default: 1000 chars, 200 overlap)
- Natural boundary splitting (sentences, paragraphs)
- Maintains context across chunks

âœ… **Milvus Vector Database Integration**
- Direct embedding and storage in Milvus
- Azure OpenAI embeddings (1536 dimensions)
- Rich metadata for filtering and tracing
- Traceable back to original source

âœ… **SOLID Architecture**
- Interface-based design (Dependency Inversion)
- Single Responsibility Principle
- Easy to test and extend
- Dependency Injection pattern

## ğŸ“ Project Structure

```
backend/wiki_ingestion/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                        # Configuration module
â”œâ”€â”€ diagnose_wiki.py                 # Diagnostic tool
â”œâ”€â”€ quickstart_milvus.sh             # Quick start script
â”œâ”€â”€ test_system.py                   # System tests
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ CLEANUP_SUMMARY.md               # Recent cleanup details
â”‚
â”œâ”€â”€ interfaces/                      # Abstractions (SOLID - Dependency Inversion)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ web_crawler.py              # IWebCrawler interface
â”‚   â”œâ”€â”€ content_extractor.py        # IContentExtractor interface
â”‚   â””â”€â”€ url_fetcher.py              # IUrlFetcher interface
â”‚
â”œâ”€â”€ models/                          # Data models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ wiki_page.py                # WikiPage model
â”‚   â””â”€â”€ wiki_chunk.py               # WikiChunk model
â”‚
â”œâ”€â”€ services/                        # Service implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ url_fetcher_service.py      # HTTP fetching with retry
â”‚   â”œâ”€â”€ content_extractor_service.py # HTML to clean content
â”‚   â”œâ”€â”€ web_crawler_service.py      # Web crawling logic
â”‚   â”œâ”€â”€ wiki_chunking_service.py    # Smart chunking
â”‚   â””â”€â”€ wiki_ingestion_orchestrator.py # Main orchestrator
â”‚
â”œâ”€â”€ examples/                        # Usage examples (Milvus only)
â”‚   â”œâ”€â”€ README.md                   # Examples documentation
â”‚   â”œâ”€â”€ ingest_to_milvus.py         # HTTP-based Milvus ingestion
â”‚   â”œâ”€â”€ ingest_private_wiki_git.py  # Git-based private wiki ingestion
â”‚   â”œâ”€â”€ simple_crawl.py             # Basic crawling example
â”‚   â””â”€â”€ verify_milvus_data.py       # Data verification tool
â”‚
â”œâ”€â”€ scripts/                         # Helper scripts
â”‚   â”œâ”€â”€ debug_wiki_url.py
â”‚   â”œâ”€â”€ search_choreo.py
â”‚   â””â”€â”€ test_auth.py
â”‚
â”œâ”€â”€ docs/                            # Documentation
â”‚   â”œâ”€â”€ guides/
â”‚   â””â”€â”€ readmes/
â”‚
â”œâ”€â”€ logs/                            # Application logs
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ utils/                           # Utility functions
```

## ğŸ—ï¸ Architecture

### SOLID Principles Applied

**1. Single Responsibility Principle (SRP)**
- Each service has one clear responsibility
- `UrlFetcherService`: Fetch URLs
- `ContentExtractorService`: Extract content
- `WebCrawlerService`: Crawl websites
- `WikiChunkingService`: Chunk content
- `WikiIngestionOrchestrator`: Coordinate workflow

**2. Open/Closed Principle (OCP)**
- Open for extension via interfaces
- Closed for modification
- Easy to add new crawler types or extraction strategies

**3. Liskov Substitution Principle (LSP)**
- Any implementation of `IWebCrawler` can replace another
- Any implementation of `IContentExtractor` can replace another

**4. Interface Segregation Principle (ISP)**
- Small, focused interfaces
- Clients depend only on what they use

**5. Dependency Inversion Principle (DIP)**
- High-level orchestrator depends on abstractions
- Low-level services implement abstractions
- Dependency injection throughout

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CRAWL WIKI                                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚    â”‚ Start URL    â”‚â”€â”€â”                                      â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                      â”‚
â”‚                      â–¼                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ WebCrawlerService (BFS)     â”‚                         â”‚
â”‚    â”‚  - Fetch page               â”‚                         â”‚
â”‚    â”‚  - Extract links            â”‚                         â”‚
â”‚    â”‚  - Queue new pages          â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                      â”‚                                      â”‚
â”‚                      â–¼                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ WikiPage objects            â”‚                         â”‚
â”‚    â”‚  - URL, title, content      â”‚                         â”‚
â”‚    â”‚  - Internal/external links  â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. EXTRACT LINKED URLS                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ Collect all URLs from pages â”‚                         â”‚
â”‚    â”‚  - Deduplicate              â”‚                         â”‚
â”‚    â”‚  - Filter out wiki pages    â”‚                         â”‚
â”‚    â”‚  - Limit to max_linked_urls â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. CHUNK WIKI PAGES                                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ WikiChunkingService         â”‚                         â”‚
â”‚    â”‚  - Smart chunking           â”‚                         â”‚
â”‚    â”‚  - Respect markdown         â”‚                         â”‚
â”‚    â”‚  - Add metadata             â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                      â”‚                                      â”‚
â”‚                      â–¼                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ WikiChunk objects           â”‚                         â”‚
â”‚    â”‚  (from wiki pages)          â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. FETCH & CHUNK LINKED CONTENT                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ For each linked URL:        â”‚                         â”‚
â”‚    â”‚  - Fetch content            â”‚                         â”‚
â”‚    â”‚  - Extract clean text       â”‚                         â”‚
â”‚    â”‚  - Chunk content            â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                      â”‚                                      â”‚
â”‚                      â–¼                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ WikiChunk objects           â”‚                         â”‚
â”‚    â”‚  (from linked content)      â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. PREPARE FOR EMBEDDING                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ Combine all chunks          â”‚                         â”‚
â”‚    â”‚  - Wiki page chunks         â”‚                         â”‚
â”‚    â”‚  - Linked content chunks    â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                      â”‚                                      â”‚
â”‚                      â–¼                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚ Ready for embedding         â”‚                         â”‚
â”‚    â”‚  {id, text, metadata}       â”‚                         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Installation

```bash
cd backend/wiki_ingestion
pip install -r requirements.txt
```

### Basic Usage

```python
from wiki_ingestion.services import (
    UrlFetcherService,
    ContentExtractorService,
    WebCrawlerService,
    WikiChunkingService,
    WikiIngestionOrchestrator
)

# Initialize services
url_fetcher = UrlFetcherService()
content_extractor = ContentExtractorService()
web_crawler = WebCrawlerService(url_fetcher, content_extractor)
chunking_service = WikiChunkingService()

# Create orchestrator
orchestrator = WikiIngestionOrchestrator(
    web_crawler=web_crawler,
    url_fetcher=url_fetcher,
    content_extractor=content_extractor,
    chunking_service=chunking_service
)

# Run ingestion
result = orchestrator.ingest_wiki(
    wiki_url='https://github.com/owner/repo/wiki',
    max_depth=2,
    max_pages=50
)

# Get chunks
chunks = result['chunks']
```

### Command Line Usage

```bash
# Set environment variables
export WIKI_URL="https://github.com/wso2/docs-choreo-dev/wiki"
export WIKI_MAX_DEPTH=2
export WIKI_MAX_PAGES=50
export WIKI_FETCH_LINKED=true

# Run ingestion
python -m backend.wiki_ingestion.main
```

### Public Wikis (HTTP-based Milvus Integration)

```bash
# Set environment variables
export WIKI_URL="https://github.com/wso2/docs-choreo-dev/wiki"
export MILVUS_URI="https://your-instance.vectordb.zillizcloud.com:19530"
export MILVUS_TOKEN="your-milvus-token"
export MILVUS_COLLECTION_NAME="choreo_developer_assistant"
export MILVUS_DIMENSION=1536
export AZURE_OPENAI_ENDPOINT="https://your-endpoint.openai.azure.com"
export AZURE_OPENAI_API_KEY="your-api-key"
export AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT="choreo-ai-embedding"

# Run HTTP-based ingestion
python -m backend.wiki_ingestion.examples.ingest_to_milvus
```

### Private Wikis (Git-based Milvus Integration)

```bash
# Set environment variables
export WIKI_URL="https://github.com/wso2-enterprise/choreo/wiki"
export GITHUB_TOKEN="your-github-token"
export MILVUS_URI="https://your-instance.vectordb.zillizcloud.com:19530"
export MILVUS_TOKEN="your-milvus-token"
export MILVUS_COLLECTION_NAME="choreo_developer_assistant"

# Run git-based ingestion for private repos
python -m backend.wiki_ingestion.examples.ingest_private_wiki_git
```

### Quick Start Script

```bash
cd backend/wiki_ingestion
./quickstart_milvus.sh
```


## ğŸ“Š Configuration

### Environment Variables

```bash
# Wiki Configuration
WIKI_URL=https://github.com/owner/repo/wiki    # Starting wiki URL
WIKI_MAX_DEPTH=2                                # Max crawl depth (0 = only start page)
WIKI_MAX_PAGES=50                               # Max pages to crawl
WIKI_FETCH_LINKED=true                          # Fetch content from linked URLs
WIKI_MAX_LINKED_URLS=0                          # 0 = unlimited, N = limit to N URLs

# Chunking Configuration
CHUNK_SIZE=1000                                 # Target chunk size (characters)
CHUNK_OVERLAP=200                               # Overlap between chunks
MIN_CHUNK_SIZE=100                              # Minimum chunk size

# Fetching Configuration
REQUEST_TIMEOUT=30                              # HTTP request timeout (seconds)
MAX_RETRIES=3                                   # Max retry attempts
BACKOFF_FACTOR=0.5                             # Retry backoff factor

# GitHub Authentication (for private repos)
GITHUB_TOKEN=your-github-token                  # GitHub personal access token

# Milvus Vector Database
MILVUS_URI=https://your-instance.vectordb.zillizcloud.com:19530
MILVUS_TOKEN=your-milvus-token
MILVUS_COLLECTION_NAME=choreo_developer_assistant
MILVUS_DIMENSION=1536                           # Embedding dimension

# Azure OpenAI (for embeddings)
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=choreo-ai-embedding
AZURE_OPENAI_EMBEDDINGS_VERSION=2024-02-01
```

### Programmatic Configuration

```python
# Custom crawler configuration
web_crawler = WebCrawlerService(
    url_fetcher=url_fetcher,
    content_extractor=content_extractor,
    max_pages=100,              # Override max pages
    respect_robots=True         # Respect robots.txt
)

# Custom chunking configuration
chunking_service = WikiChunkingService(
    chunk_size=1500,            # Larger chunks
    chunk_overlap=300,          # More overlap
    min_chunk_size=200          # Higher minimum
)

# Custom orchestrator configuration
orchestrator = WikiIngestionOrchestrator(
    web_crawler=web_crawler,
    url_fetcher=url_fetcher,
    content_extractor=content_extractor,
    chunking_service=chunking_service,
    fetch_linked_content=True,  # Enable linked content
    max_linked_urls=100         # More linked URLs
)
```

## ğŸ” Data Models

### WikiPage

Represents a single wiki page.

```python
@dataclass
class WikiPage:
    url: str                        # Page URL
    title: str                      # Page title
    content: str                    # Clean text content
    raw_html: str                   # Original HTML
    markdown: str                   # Markdown format
    internal_urls: Set[str]         # URLs within same wiki
    external_urls: Set[str]         # External URLs
    repository: str                 # e.g., "wso2/choreo"
    owner: str                      # e.g., "wso2"
    wiki_name: str                  # Wiki name
    page_path: str                  # Path within wiki
    fetched_at: datetime            # When fetched
    metadata: Dict[str, Any]        # Additional metadata
    depth: int                      # Crawl depth
    parent_url: str                 # Parent page URL
```

### WikiChunk

Represents a chunk of content ready for embedding.

```python
@dataclass
class WikiChunk:
    chunk_id: str                   # Unique chunk ID
    text: str                       # Chunk text
    chunk_index: int                # Index within source
    source_url: str                 # Source page URL
    source_title: str               # Source page title
    source_type: str                # 'wiki_page' or 'linked_content'
    repository: str                 # Repository
    owner: str                      # Owner
    chunk_size: int                 # Size in characters
    total_chunks: int               # Total chunks from source
    metadata: Dict[str, Any]        # Additional metadata
    created_at: datetime            # Creation time
```

## ğŸ§ª Testing

### Unit Tests

```bash
# Run all tests
pytest backend/wiki_ingestion/tests/

# Run specific test file
pytest backend/wiki_ingestion/tests/test_web_crawler.py

# Run with coverage
pytest --cov=backend.wiki_ingestion backend/wiki_ingestion/tests/
```

### Manual Testing

```python
# Test URL fetching
from wiki_ingestion.services import UrlFetcherService

fetcher = UrlFetcherService()
html = fetcher.fetch('https://github.com/wso2/docs-choreo-dev/wiki')
print(f"Fetched {len(html)} characters")

# Test content extraction
from wiki_ingestion.services import ContentExtractorService

extractor = ContentExtractorService()
content = extractor.extract_content(html, url)
print(f"Title: {content['title']}")
print(f"Content: {content['content'][:200]}...")

# Test chunking
from wiki_ingestion.services import WikiChunkingService

chunker = WikiChunkingService()
chunks = chunker.chunk_page(page)
print(f"Created {len(chunks)} chunks")
```

## ğŸ“ˆ Performance

### Crawling Speed

- **Average**: 2-3 pages/second
- **With linked content**: 1-2 pages/second
- **Respects rate limits**: Automatic backoff

### Memory Usage

- **Per page**: ~50-200 KB
- **Per chunk**: ~1-5 KB
- **Efficient**: Processes in batches

### Recommendations

```python
# For large wikis (100+ pages)
max_pages=100
max_depth=2
fetch_linked_content=False  # Or limit max_linked_urls

# For complete ingestion
max_pages=None  # No limit
max_depth=3
fetch_linked_content=True
max_linked_urls=200
```

## ğŸ› ï¸ Extending the System

### Custom Crawler Implementation

```python
from wiki_ingestion.interfaces import IWebCrawler

class CustomCrawler(IWebCrawler):
    def crawl(self, start_url: str, max_depth: int = 3):
        # Your custom crawling logic
        pass
    
    def extract_page(self, url: str):
        # Your custom extraction logic
        pass

# Use it
custom_crawler = CustomCrawler()
orchestrator = WikiIngestionOrchestrator(
    web_crawler=custom_crawler,  # Inject custom implementation
    # ... other services
)
```

### Custom Content Extractor

```python
from wiki_ingestion.interfaces import IContentExtractor

class CustomExtractor(IContentExtractor):
    def extract_content(self, html: str, url: str):
        # Your custom extraction logic
        pass

# Use it
custom_extractor = CustomExtractor()
web_crawler = WebCrawlerService(
    url_fetcher=url_fetcher,
    content_extractor=custom_extractor  # Inject custom implementation
)
```

## ğŸ› Troubleshooting

### Issue: Crawler not finding pages

**Solution**: Check URL pattern matching
```python
# Debug URL validation
crawler = WebCrawlerService(...)
url = "https://github.com/owner/repo/wiki/Page"
is_valid = crawler.is_valid_url(url, base_domain)
print(f"URL valid: {is_valid}")
```

### Issue: Content extraction failing

**Solution**: Check HTML structure
```python
# Debug content extraction
extractor = ContentExtractorService()
content = extractor.extract_content(html, url)
print(f"Extracted: {content}")
```

### Issue: Too many chunks

**Solution**: Increase chunk size
```python
chunker = WikiChunkingService(
    chunk_size=2000,  # Larger chunks
    min_chunk_size=500  # Higher minimum
)
```

## ğŸ“ License

Same as parent project.

## ğŸ¤ Contributing

1. Follow SOLID principles
2. Add tests for new features
3. Update documentation
4. Use type hints
5. Follow existing code style

## ğŸ“ Support

For issues or questions, please open an issue in the main repository.

---

**Built with â¤ï¸ using SOLID principles**

