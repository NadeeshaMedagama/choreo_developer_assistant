# Memory Lag Fix - November 4, 2025

## Problem Identified

The program was freezing/lagging when RAM usage reached ~87-90% during the chunking and embedding generation phase. The logs showed:

```
2025-11-04 08:24:30 - backend.services.ingestion - INFO - üìù Chunking README.md (6794 chars)...
[PROGRAM HANGS HERE]
```

## Root Causes

1. **Unreachable Code Bug**: Line 233 had `logger.info(f"Found README.md file...")` AFTER a return statement, preventing README processing from continuing properly.

2. **Memory Buildup During Embedding Generation**: 
   - Azure OpenAI batch size was too large (10 items)
   - No immediate memory cleanup after chunking
   - Garbage collection only happened every 50 batches
   - The original content string wasn't freed immediately after chunking

3. **Insufficient Memory Management**:
   - Content was held in memory through the entire chunking process
   - No aggressive garbage collection between operations
   - Azure OpenAI response objects weren't being freed immediately

## Solutions Implemented

### 1. Fixed Unreachable Code (ingestion.py, line 233)
**Before:**
```python
if not readme_file:
    return {...}
    logger.info(f"Found README.md file: {readme_file['path']}")  # Never executed!
```

**After:**
```python
if not readme_file:
    return {...}

logger.info(f"Found README.md file: {readme_file['path']}")  # Now executes!
```

### 2. Immediate Memory Cleanup After Chunking (ingestion.py, ~line 361)
**Added:**
```python
chunks = self.chunker.chunk_text(content, file_metadata)
logger.info(f"‚úì Created {len(chunks)} chunks from {readme_file['name']}")

# **IMMEDIATE MEMORY CLEANUP** after chunking
del content  # Free the original content immediately
force_garbage_collection()
logger.info(f"Memory after chunking: {get_memory_usage()}")
```

### 3. Reduced Azure OpenAI Batch Size (llm_service.py, line 160)
**Changed:**
```python
# From: batch_size = 10
batch_size = 5  # REDUCED to 5 for better memory management
```

### 4. Aggressive Garbage Collection (llm_service.py, line 168-175)
**Added immediate cleanup after EVERY batch:**
```python
batch_embeddings = [item.embedding for item in response.data]
embeddings.extend(batch_embeddings)

# **IMMEDIATE CLEANUP** - Free response object
del response, batch_embeddings

# Force garbage collection after EVERY batch (not just every 50)
gc.collect()
```

### 5. Better Progress Logging
**Added transparency so users know the program is working:**
```python
logger.info(f"Memory after chunking: {get_memory_usage()}")
logger.debug(f"Processed {i + len(batch)}/{len(texts)} embeddings")
```

## Expected Results

‚úÖ **No more freezing** - Memory is freed immediately after each operation
‚úÖ **Visible progress** - Logs show memory usage and progress
‚úÖ **Lower peak memory** - Batch size reduced from 10 to 5
‚úÖ **Faster recovery** - Garbage collection after every batch instead of every 50

## Memory Management Strategy

1. **Chunking Phase**: 
   - Create chunks
   - Immediately delete original content
   - Force garbage collection
   - Log memory usage

2. **Embedding Phase** (batches of 2 chunks at a time):
   - Generate embeddings for small batch (5 texts)
   - Store in Pinecone immediately
   - Delete batch variables
   - Force garbage collection
   - Repeat

3. **Between Files**:
   - Clear model cache
   - Check total memory usage
   - Reinitialize model if > 800MB

## Testing Instructions

Run the ingestion script and watch for these new log messages:

```bash
cd /home/nadeeshame/CHOREO/Choreo\ AI\ Assistant/choreo-ai-assistant/backend
python run_ingestion.py
```

Expected log output:
```
2025-11-04 XX:XX:XX - INFO - üìù Chunking README.md (6794 chars)...
2025-11-04 XX:XX:XX - INFO - ‚úì Created 7 chunks from README.md
2025-11-04 XX:XX:XX - INFO - Memory after chunking: 45.2MB (66.3%)  ‚Üê NEW!
2025-11-04 XX:XX:XX - INFO - üîÑ Processing 7 chunks in batches of 2...
2025-11-04 XX:XX:XX - INFO -   Batch 1/4: Generating embeddings... [Memory: 46.1MB (67.5%)]
2025-11-04 XX:XX:XX - INFO -   ‚úì Generated 2 embeddings
2025-11-04 XX:XX:XX - INFO -   üíæ Storing embeddings in Pinecone...
2025-11-04 XX:XX:XX - INFO -   ‚úì Stored batch 1/4 (2 embeddings)
```

## Performance Improvements

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Batch Size (Azure) | 10 | 5 | 50% reduction |
| Garbage Collection | Every 50 batches | Every batch | 50x more frequent |
| Memory Cleanup | After all chunks | After chunking immediately | Instant |
| Peak Memory | ~87.7% (freeze) | ~70% (estimated) | ~20% reduction |

## Files Modified

1. `/backend/services/ingestion.py` - Fixed unreachable code, added immediate cleanup
2. `/backend/services/llm_service.py` - Reduced batch size, aggressive GC

