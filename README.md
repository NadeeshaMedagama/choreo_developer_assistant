# DevChoreo (Choreo AI Assistant)

Minimal RAG assistant that ingests a GitHub repo into Milvus and answers with Azure OpenAI. Frontend is a ChatGPT-like UI built with React + Vite + Tailwind.
> See [Documentation Index](./docs/readmes/INDEX.md) for complete guides on setup, features, and deployment.
> üìö **All documentation is now centralized in [`docs/readmes/`](./docs/readmes/INDEX.md)**  
> See [Documentation Index](./docs/readmes/INDEX.md) for complete guides on setup, features, and deployment.
- Frontend: React, Vite, Tailwind CSS
## Stack
- Backend: FastAPI, Azure OpenAI, Milvus, LangChain, LangGraph
- Frontend: React, Vite, Tailwind CSS
- Monitoring: Prometheus, Grafana, Alertmanager, Structured Logging
- Advanced Features: Conversation Memory with Smart Summarization, Progressive Streaming Responses, Context-Aware Retrieval

## ‚ú® Key Features

### üß† Intelligent Conversation Memory
- **Automatic Summarization**: LLM-powered summaries when conversations exceed token limits
- **Smart Context Management**: Keeps recent messages (last 6) fully detailed while summarizing older ones
- **Metadata Extraction**: Tracks topics, key questions, and important decisions
- **Token Tracking**: Real-time monitoring of conversation size
- **Graceful Fallback**: Simple summaries if LLM is unavailable during peak times

### ‚ö° Progressive Streaming Responses
- **Real-time Streaming**: Answers appear word-by-word like ChatGPT/Gemini
- **Visual Feedback**: Blinking cursor indicator during streaming
- **Automatic Fallback**: Switches to standard API if streaming fails
- **Better UX**: First token in 1-2 seconds vs 3-5 for full response

### üîç Context-Aware Retrieval
- **Enhanced Query Enrichment**: Uses conversation history to improve database searches
- **Better Results**: Retrieves more relevant chunks from Milvus
- **Summary Integration**: Includes conversation summary in retrieval context
- **Quality Filtering**: Score-based filtering to ensure high-quality results

### üö´ Intelligent Content Filtering
- **OpenChoreo Exclusion**: Automatically filters out non-Choreo platform content
- **Multi-stage Filtering**: Applied to retrieval, context, and source display
- **Clean Answers**: Ensures responses are based only on WSO2 Choreo platform

### üîó URL Validation
- **Automatic Validation**: Validates all URLs in answers and sources before displaying
- **404 Prevention**: Removes broken or inaccessible URLs from responses
- **Concurrent Checks**: Validates multiple URLs in parallel for performance
- **Smart Caching**: Caches validation results to avoid redundant checks
- **Configurable**: Enable/disable validation and adjust timeout settings

### üìä Production Monitoring
- **23+ Metrics**: Infrastructure, application, AI, vector DB, and ingestion metrics
- **Pre-built Dashboard**: Grafana dashboard with 8 key panels
- **Smart Alerts**: 7 alert rules for proactive issue detection
- **Structured Logging**: JSON logs with automatic rotation
- **One-click Access**: Monitoring button integrated into UI

### üîÑ Incremental Ingestion
- **Smart Chunking**: Avoids re-processing already ingested files
- **Change Detection**: Only processes new or modified content
- **Performance**: Significantly faster re-ingestion of large repositories
- **Metadata Tracking**: Stores ingestion status and timestamps

### üé® Modern UI/UX
- **ChatGPT-like Interface**: Familiar chat experience
- **Multiple Chats**: Create, switch, rename, and delete conversations
- **Persistent History**: Conversations saved in localStorage
- **Source Citations**: View document sources with relevance scores
- **Edit & Copy**: Edit questions and copy responses easily

## Quick Start

1. Open http://localhost:5173
2. Type a question: "What is Choreo?"
3. Get AI-powered answers with context!
4. Enjoy progressive streaming responses (like ChatGPT)
5. Experience intelligent conversation memory across long chats

**Key Features:**
- üéØ **Smart Conversation Memory**: Automatic summarization when conversations get long
- ‚ö° **Progressive Streaming**: Answers appear word-by-word in real-time
- üîç **Context-Aware Retrieval**: Uses conversation history to improve search results
- üö´ **Content Filtering**: Automatically excludes OpenChoreo content
- üìä **Memory Stats**: See token usage and summarization status

**Using the API:**
```bash
# Standard RAG query with conversation history
curl -X POST "http://localhost:8000/api/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How do I deploy a service?",
    "conversation_history": [
      {"role": "user", "content": "What is Choreo?"},
      {"role": "assistant", "content": "Choreo is..."}
    ],
    "enable_summarization": true
  }'

# Streaming responses (progressive like ChatGPT)
curl -X POST "http://localhost:8000/api/ask/stream?question=What%20is%20Choreo%3F"

# LangGraph-based query (advanced)
curl -X POST "http://localhost:8000/api/ask_graph?question=What%20is%20Choreo%3F"
```
## üê≥ Docker Deployment
- **[Run Project](./docs/readmes/RUN_PROJECT.md)** - How to run the application
### Quick Start with Docker Compose
---

cd docker

# Create .env file with your credentials
cp ../.env.example .env
# Edit .env with your API keys

# Build and run
docker-compose up --build

# Run in background
docker-compose up -d

**Setup in GitHub:**
**Access Points:**
- Backend API: http://localhost:8000
- Frontend UI: http://localhost:3000
- API Docs: http://localhost:8000/docs
2. **Payload URL**: `https://your-domain.com/api/webhook/github`
**See [Docker Guide](./docs/readmes/DOCKER_README.md) for production deployment.**
5. **Secret**: (optional, not currently verified)
6. Click **Add webhook**

## üö¢ Choreo Platform Deployment

This project is ready for deployment to WSO2 Choreo platform:
#### Features & Capabilities
- **[Conversation Memory](./docs/readmes/CONVERSATION_MEMORY_IMPLEMENTATION.md)** - Smart summarization for long conversations
- **[Streaming Responses](./docs/readmes/STREAMING_IMPLEMENTATION.md)** - Progressive ChatGPT-like streaming
- **[Incremental Ingestion](./docs/readmes/INCREMENTAL_INGESTION.md)** - Smart chunking feature
- **[Content Filtering](./docs/readmes/FIX_OPENCHOREO_FILTERING.md)** - Excludes non-Choreo content
- **[429 Error Handling](./docs/readmes/TROUBLESHOOTING_429_ERRORS.md)** - Azure OpenAI rate limit solutions
# 1. Review deployment configuration
cat .choreo/component.yaml
cat .choreo/openapi.yaml

# 2. Commit and push to GitHub
git add .
git commit -m "Deploy to Choreo"
git push origin main

# 3. Deploy in Choreo Console
# - Create new component
# - Connect GitHub repository
# - Component Directory: . (root)
# - Add environment variables from Choreo Secrets
- **[Test Files](./backend/tests/README.md)** - Testing documentation
- **[Scripts Documentation](./backend/scripts/README.md)** - Development scripts
**Complete guides:**
- [Choreo Deployment Guide](./docs/readmes/CHOREO_DEPLOYMENT.md)
- [Choreo Quick Start](./CHOREO_QUICK_START.md)
- [OpenAPI Specification](./.choreo/README.md)

#### Security & Compliance
- **[Security Audit Report](docs/implementation/SECURITY_AUDIT_REPORT.md)** - Security verification

---

## üß† Conversation Memory System

DevChoreo includes an intelligent **Conversation Memory Management System** that maintains context across long conversations while staying within token limits.

### How It Works

1. **Normal Conversations** (Below token limit)
   - All conversation history sent to LLM
   - Full context preserved

2. **Long Conversations** (Exceeding limits)
   - Older messages automatically summarized by LLM
   - Recent messages (last 6) kept fully detailed
   - Summary + recent messages sent to LLM
   - Enhanced context for better answers

### Key Features

- ‚úÖ **Automatic Summarization**: LLM creates intelligent summaries when needed
- ‚úÖ **Token Management**: Tracks conversation size in real-time
- ‚úÖ **Metadata Extraction**: Captures topics, key questions, and decisions
- ‚úÖ **Context-Aware Retrieval**: Uses history to improve database searches
- ‚úÖ **Configurable**: Control limits and enable/disable per request
- ‚úÖ **Graceful Fallback**: Simple summaries if LLM unavailable

### Configuration

**Environment Variables:**
```bash
# Enable/disable LLM summarization (useful during peak times)
ENABLE_LLM_SUMMARIZATION=true

# Maximum summarization retries before fallback
MAX_SUMMARIZATION_RETRIES=2
```

**Per-Request Control:**
```json
{
  "question": "How do I deploy?",
  "conversation_history": [...],
  "enable_summarization": true,
  "max_history_tokens": 4000
}
```

**Response Includes:**
```json
{
  "answer": "...",
  "memory_stats": {
    "total_tokens": 3200,
    "summarized_count": 2,
    "summary_created": true
  },
  "summary": {
    "content": "User learned about...",
    "topics_covered": ["deployment", "APIs"],
    "key_questions": ["How to deploy?"],
    "important_decisions": ["Use GitHub integration"]
  }
}
```

### Documentation

- **[Implementation Guide](./docs/readmes/CONVERSATION_MEMORY_IMPLEMENTATION.md)**
- **[Quick Start](./docs/readmes/QUICK_START_CONVERSATION_MEMORY.md)**
- **[Visual Guide](./docs/readmes/VISUAL_GUIDE.md)**
- **[Service Documentation](./backend/services/CONVERSATION_MEMORY_README.md)**
- **[Troubleshooting 429 Errors](./docs/readmes/TROUBLESHOOTING_429_ERRORS.md)**

---

## ‚ö° Progressive Streaming Responses

DevChoreo delivers answers progressively, like ChatGPT and Gemini, for better user experience.

### Features

- ‚úÖ **Real-time Streaming**: Answers appear word-by-word as generated
- ‚úÖ **Streaming Cursor**: Blinking indicator shows active streaming
- ‚úÖ **Graceful Fallback**: Auto-switches to regular API if streaming fails
- ‚úÖ **Works Everywhere**: New messages, regenerate, and conversation history

### How to Use

**Frontend (Automatic):**
- Just ask a question - streaming is enabled by default
- Watch the answer appear progressively with blinking cursor

**API (Manual):**
```bash
# Streaming endpoint
curl -N -X POST "http://localhost:8000/api/ask/stream?question=What%20is%20Choreo%3F"

# Returns Server-Sent Events (SSE)
data: {"content": "Choreo "}
data: {"content": "is "}
data: {"content": "a "}
...
data: [DONE]
```

### Performance

- **First Token**: 1-2 seconds (vs 3-5 for full response)
- **Perceived Speed**: Much faster user experience
- **Total Time**: Similar to non-streaming
- **Network**: More efficient with progressive data

### Documentation

- **[Streaming Implementation](./docs/readmes/STREAMING_IMPLEMENTATION.md)**
- **[Streaming Responses Guide](./docs/readmes/STREAMING_RESPONSES.md)**
- **[URL Validation](./docs/readmes/URL_VALIDATION.md)**

---

## üö´ Content Filtering

DevChoreo automatically filters out non-Choreo content to ensure accurate answers.

### What's Filtered

- ‚ùå **OpenChoreo repositories** - Excluded from context and sources
- ‚úÖ **WSO2 Choreo only** - Answers based exclusively on Choreo platform

### Where Filtering Applies

1. **Vector DB Retrieval**: OpenChoreo content excluded from search results
2. **LLM Context**: Filtered content never sent to AI
3. **Source Display**: Only Choreo sources shown to users
4. **System Prompts**: Clear instructions to avoid non-Choreo info

### Configuration

Filtering is automatic and enabled by default. The system:
- Checks repository metadata for "openchoreo" references
- Excludes matching content at multiple pipeline stages
- Ensures clean, relevant answers

**See:** [Content Filtering Guide](./docs/readmes/FIX_OPENCHOREO_FILTERING.md)

---

## üîÑ How It All Works Together

DevChoreo uses a sophisticated pipeline that combines conversation memory, context-aware retrieval, and intelligent filtering to deliver accurate answers.

### Complete Query Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. User asks: "How do I monitor my deployment?"                     ‚îÇ
‚îÇ    + Conversation history (previous 10 messages)                     ‚îÇ
‚îÇ    + Existing summary (if conversation is long)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. CONVERSATION MEMORY MANAGER                                       ‚îÇ
‚îÇ    ‚îú‚îÄ Estimate tokens in history: 3,500 tokens                      ‚îÇ
‚îÇ    ‚îú‚îÄ Check if > 75% of limit (4,000): YES                          ‚îÇ
‚îÇ    ‚îú‚îÄ Split: Older messages (4) + Recent messages (6)               ‚îÇ
‚îÇ    ‚îú‚îÄ Summarize older messages with LLM                             ‚îÇ
‚îÇ    ‚îÇ  "User learned about Choreo basics, created a project,         ‚îÇ
‚îÇ    ‚îÇ   deployed a service, and discussed authentication."           ‚îÇ
‚îÇ    ‚îî‚îÄ Output: Summary + Recent Messages                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. QUERY ENRICHMENT                                                  ‚îÇ
‚îÇ    Build enriched query for better retrieval:                        ‚îÇ
‚îÇ    ‚îú‚îÄ Summary: "User learned about Choreo basics..."                ‚îÇ
‚îÇ    ‚îú‚îÄ Recent context: [last 4 messages]                             ‚îÇ
‚îÇ    ‚îî‚îÄ Current question: "How do I monitor my deployment?"           ‚îÇ
‚îÇ    Result: "Summary: User learned... Recent: [context]              ‚îÇ
‚îÇ            Current question: How do I monitor my deployment?"        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. VECTOR DB RETRIEVAL (Milvus Cloud)                               ‚îÇ
‚îÇ    ‚îú‚îÄ Convert enriched query to embeddings (Azure OpenAI)           ‚îÇ
‚îÇ    ‚îú‚îÄ Search Milvus for similar chunks (top 10)                     ‚îÇ
‚îÇ    ‚îú‚îÄ Filter out OpenChoreo content                                 ‚îÇ
‚îÇ    ‚îú‚îÄ Apply quality filtering (score > 0.7)                         ‚îÇ
‚îÇ    ‚îî‚îÄ Return: 5-10 high-quality context chunks                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. BUILD LLM CONTEXT                                                 ‚îÇ
‚îÇ    Combine all context for the LLM:                                  ‚îÇ
‚îÇ    ‚îú‚îÄ System Prompt: "You are DevChoreo, assistant for Choreo..."   ‚îÇ
‚îÇ    ‚îú‚îÄ Conversation Summary (if exists): "User learned about..."     ‚îÇ
‚îÇ    ‚îú‚îÄ Knowledge Base Context: [5-10 relevant chunks from Milvus]    ‚îÇ
‚îÇ    ‚îú‚îÄ Recent Messages: [last 6 messages fully detailed]             ‚îÇ
‚îÇ    ‚îî‚îÄ Current Question: "How do I monitor my deployment?"           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. LLM PROCESSING (Azure OpenAI)                                    ‚îÇ
‚îÇ    ‚îú‚îÄ Stream response word-by-word (if using /api/ask/stream)       ‚îÇ
‚îÇ    ‚îú‚îÄ OR return complete answer (if using /api/ask)                 ‚îÇ
‚îÇ    ‚îî‚îÄ Generate answer based on full context                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. RESPONSE TO USER                                                  ‚îÇ
‚îÇ    ‚îú‚îÄ Answer: "To monitor your deployment in Choreo..."             ‚îÇ
‚îÇ    ‚îú‚îÄ Sources: [Filtered Choreo docs with scores]                   ‚îÇ
‚îÇ    ‚îú‚îÄ Memory Stats: {total_tokens: 3200, summarized: 4}            ‚îÇ
‚îÇ    ‚îî‚îÄ Updated Summary: [For next question]                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Benefits of This Architecture

1. **Better Context Understanding**
   - Summary preserves conversation history without token bloat
   - Recent messages ensure precision for follow-ups
   - Enriched queries improve retrieval relevance

2. **Accurate Answers**
   - Multi-stage filtering ensures quality
   - Conversation-aware retrieval finds better chunks
   - LLM has full context (summary + recent + knowledge base)

3. **Token Efficiency**
   - Automatic summarization when limits approached
   - Only recent messages kept in full detail
   - Graceful degradation during peak times

4. **User Experience**
   - Progressive streaming for faster perceived response
   - Memory stats show token usage
   - Transparent source citations

### Example Conversation Flow

**Turn 1:**
- User: "What is Choreo?"
- System: Full answer + saves to history
- Memory: 1 message, 150 tokens

**Turn 2-6:**
- User asks about projects, deployment, APIs, etc.
- System: Uses full history for context
- Memory: 12 messages, 2,800 tokens

**Turn 7 (Trigger summarization):**
- User: "How do I monitor my deployment?"
- System: Detects 3,500 tokens (>75% limit)
- Action: Summarizes messages 1-6, keeps 7-12 recent
- Memory: Summary (200 tokens) + 6 messages (1,500 tokens) = 1,700 tokens
- Result: Accurate answer with 50% token reduction

**Turn 8+:**
- System continues with summary + recent messages
- Can handle much longer conversations efficiently

---

## üì° API Reference

### Health & Status

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Welcome message |
| `/health` | GET | Health check (legacy) |
| `/api/health` | GET | Health check with Milvus status |

### AI Query Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/ask` | POST | Ask question (standard RAG) |
| `/api/ask/stream` | POST | Ask question with progressive streaming |
| `/api/ask_graph` | POST | Ask question (LangGraph RAG) |
| `/ask` | POST | Legacy ask endpoint |
| `/ask_graph` | POST | Legacy graph endpoint |

### Data Ingestion

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/ingest/github` | POST | Ingest single repository |
| `/api/ingest/github/with-images` | POST | Ingest with image processing |
| `/api/ingest/org` | POST | Bulk ingest organization repos |
| `/ingest/github` | POST | Legacy ingest endpoint |

### Webhooks

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/webhook/github` | POST | GitHub push webhook handler |

**Complete API Documentation:**
- Interactive Docs: http://localhost:8000/docs
- OpenAPI Spec: [.choreo/openapi.yaml](./.choreo/openapi.yaml)
- API Guide: [.choreo/README.md](./.choreo/README.md)
### Backend
- **FastAPI** - High-performance async API framework
- **Azure OpenAI** - GPT-4 for chat, text-embedding-ada-002 for embeddings
- **Milvus Cloud** - Serverless vector database for semantic search
- **LangChain** - LLM orchestration framework
- **LangGraph** - Advanced graph-based RAG workflows
- **Python 3.12+** - Modern Python with type hints

### Frontend
- **React 18** - Modern component-based UI
- **Vite** - Lightning-fast build tool
- **Tailwind CSS** - Utility-first styling
## üìÅ Project Structure

```
choreo-ai-assistant/
‚îú‚îÄ‚îÄ .choreo/                    # Choreo deployment configuration
‚îÇ   ‚îú‚îÄ‚îÄ component.yaml         # Component specification
‚îÇ   ‚îú‚îÄ‚îÄ openapi.yaml           # API documentation
‚îÇ   ‚îî‚îÄ‚îÄ README.md              # OpenAPI guide
‚îú‚îÄ‚îÄ backend/                   # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ app.py                 # Main application
‚îÇ   ‚îú‚îÄ‚îÄ services/              # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Azure OpenAI service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context_manager.py # Vector DB context management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_memory_manager.py  # ‚≠ê Smart summarization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_service.py  # GitHub integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_service.py   # Google Vision API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py       # Data ingestion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_graph.py       # LangGraph workflows
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CONVERSATION_MEMORY_README.md  # Memory system docs
‚îÇ   ‚îú‚îÄ‚îÄ db/                    # Database clients
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_client.py   # Milvus client
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/            # Monitoring system ‚≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py         # Prometheus metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging_config.py  # Logging setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alerts.py          # Alert rules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml     # Prometheus config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana_dashboard.json  # Pre-built dashboard
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml # Monitoring stack
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ install.sh         # Installation script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ start.sh           # Start all services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stop.sh            # Stop all services
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docs/              # Monitoring guides
‚îÇ   ‚îú‚îÄ‚îÄ tests/                 # Test files
‚îÇ   ‚îî‚îÄ‚îÄ scripts/               # Development scripts
‚îÇ       ‚îú‚îÄ‚îÄ debug/             # Debug tools
‚îÇ       ‚îú‚îÄ‚îÄ fetch/             # Data fetching
‚îÇ       ‚îî‚îÄ‚îÄ ingest/            # Data ingestion
‚îú‚îÄ‚îÄ frontend/                  # React frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/                   # Source code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx            # Main app with streaming support
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/        # UI components
‚îÇ   ‚îî‚îÄ‚îÄ public/                # Static assets
‚îú‚îÄ‚îÄ diagram_processor/         # Diagram/image processing
‚îú‚îÄ‚îÄ data/                      # Data files
‚îú‚îÄ‚îÄ docs/                      # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ readmes/               # ‚≠ê Detailed guides
‚îÇ       ‚îú‚îÄ‚îÄ CONVERSATION_MEMORY_IMPLEMENTATION.md
‚îÇ       ‚îú‚îÄ‚îÄ STREAMING_IMPLEMENTATION.md
‚îÇ       ‚îú‚îÄ‚îÄ TROUBLESHOOTING_429_ERRORS.md
‚îÇ       ‚îú‚îÄ‚îÄ FIX_OPENCHOREO_FILTERING.md
‚îÇ       ‚îî‚îÄ‚îÄ INDEX.md           # Documentation index
‚îú‚îÄ‚îÄ docker/                    # Docker configuration
‚îú‚îÄ‚îÄ logs/                      # Application logs (auto-generated)
‚îÇ   ‚îú‚îÄ‚îÄ app.log                # All logs
‚îÇ   ‚îú‚îÄ‚îÄ error.log              # Errors only
‚îÇ   ‚îú‚îÄ‚îÄ ai.log                 # AI operations
‚îÇ   ‚îî‚îÄ‚îÄ ingestion.log          # Ingestion logs
‚îú‚îÄ‚îÄ Dockerfile                 # Production container
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md                  # This file
```

---

## üß™ Testing

### Run Backend Tests

```bash
# Test backend initialization
python backend/tests/test_backend.py

# Test GitHub connectivity
python backend/tests/test_github.py

# Test chunking functionality
python backend/tests/test_chunking.py
```

### Run Debug Scripts

```bash
# Debug GitHub access
python backend/scripts/debug/debug_github_access.py

# Check repository visibility
python backend/scripts/debug/debug_github_repos.py
```

**See [Testing Guide](./backend/tests/README.md) for comprehensive testing documentation.**

---

## üîí Security Best Practices

### ‚úÖ What's Protected

- ‚úÖ All credentials in `.gitignore` (never committed)
- ‚úÖ Environment variables in `backend/.env` (gitignored)
- ‚úÖ API keys managed through Choreo Secrets in production
- ‚úÖ No hardcoded credentials in source code
- ‚úÖ Security audit completed (see [SECURITY_AUDIT_REPORT.md](docs/implementation/SECURITY_AUDIT_REPORT.md))

### ‚ö†Ô∏è Important Security Notes

1. **Never commit** `backend/.env` or credential files
2. **Use Choreo Secrets** for production deployment
3. **Rotate API keys** regularly
4. **Limit API permissions** to minimum required
5. **Review changes** before pushing to GitHub

**Security Audit:** [SECURITY_AUDIT_REPORT.md](docs/implementation/SECURITY_AUDIT_REPORT.md)

---

## üîß Troubleshooting

### Azure OpenAI Errors

**Issue:** `401 Unauthorized` or `Invalid API key`
- ‚úÖ Verify `AZURE_OPENAI_API_KEY` is correct
- ‚úÖ Check `AZURE_OPENAI_ENDPOINT` URL format
- ‚úÖ Ensure deployment names match your Azure resources
- ‚úÖ Confirm API version is supported

### Milvus Connection Issues

**Issue:** `Milvus: disconnected` in health check
- ‚úÖ Verify `MILVUS_URI` is valid and accessible
- ‚úÖ Check `MILVUS_TOKEN` is correct
- ‚úÖ Ensure collection name exists: `MILVUS_COLLECTION_NAME`
- ‚úÖ Verify network connectivity to Milvus Cloud
- ‚úÖ Check dimension settings match your embedding model

### Frontend API Errors

**Issue:** `Network Error` or `Failed to fetch`
- ‚úÖ Confirm backend is running on port 8000
- ‚úÖ Check Vite proxy configuration in `vite.config.js`
- ‚úÖ Verify CORS settings in `backend/app.py`
- ‚úÖ Ensure both frontend and backend are running

### Import Errors

**Issue:** `ModuleNotFoundError: No module named 'backend'`
- ‚úÖ Activate virtual environment: `source .venv/bin/activate`
- ‚úÖ Install dependencies: `pip install -r requirements.txt`
- ‚úÖ Run from project root directory
- ‚úÖ Check `PYTHONPATH` if using custom setup

### Google Vision API Issues

**Issue:** `Google Vision API not configured`
- ‚úÖ Add `GOOGLE_CREDENTIALS_JSON` to `backend/.env`
- ‚úÖ See [Google Credentials Setup](./docs/readmes/GOOGLE_CREDENTIALS_SETUP.md)
- ‚úÖ Verify service account has Vision API permissions
- ‚úÖ Check JSON format is valid

**More Help:**
- [Setup Guide](./docs/readmes/SETUP_GUIDE.md)
- [Troubleshooting Guide](./docs/readmes/CRASH_ANALYSIS_AND_FIXES.md)
- [Documentation Index](./docs/readmes/INDEX.md)
- **GitHub Integration** - Webhook-based auto-updates
- **Docker** - Containerized deployment

## üìä Performance & Monitoring

### üéØ Comprehensive Monitoring System

DevChoreo includes a **complete production-ready monitoring stack** with Prometheus, Grafana, and structured logging. Get real-time insights into your application's performance, health, and resource usage.

#### Quick Start

```bash
# Install monitoring tools (Prometheus + Grafana)
cd backend/monitoring
./install.sh

# Start all monitoring services
./start.sh

# Run load test to generate metrics
./load_test.sh

# Access monitoring dashboard
# Click the monitoring icon (üìä) in the bottom-right corner of DevChoreo UI
# OR visit: http://localhost:3000 (Grafana - admin/admin)
```

#### Features

- ‚úÖ **23+ Metrics Types**: Infrastructure, application, AI-specific, vector DB, and GitHub ingestion metrics
- ‚úÖ **Pre-configured Dashboard**: Beautiful Grafana dashboard with 8 key panels
- ‚úÖ **Smart Alerting**: 7 alert rules for proactive issue detection
- ‚úÖ **Structured Logging**: JSON logs with rotation (app, errors, AI ops, ingestion)
- ‚úÖ **One-Click Access**: Monitoring button integrated into DevChoreo UI
- ‚úÖ **Docker Support**: Full stack deployable via docker-compose
- ‚úÖ **Production Ready**: Environment-aware configuration for local/Choreo deployment

#### Metrics Collected

**Infrastructure (8 metrics)**
- CPU, memory, disk usage
- Process count and system health

**Application (4 metrics)**
- HTTP requests (by method, endpoint, status)
- Request duration and active requests
- Error tracking by type

**AI-Specific (4 metrics)**
- Inference duration and success rate
- Token usage (input/output)
- Payload sizes

**Vector Database (3 metrics)**
- Search duration and operations
- Results count distribution

**GitHub/Ingestion (3 metrics)**
- Ingestion duration and success rate
- Files processed by type

**Health (1 metric)**
- Component health status

#### Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| **Metrics Endpoint** | http://localhost:8000/metrics | - |
| **Prometheus** | http://localhost:9090 | - |
| **Grafana Dashboard** | http://localhost:3000 | admin/admin |
| **Alertmanager** | http://localhost:9093 | - |

#### Monitoring Documentation

- **[Setup Guide](./backend/monitoring/docs/SETUP_GUIDE.md)** - Detailed installation and configuration
- **[Quick Reference](./backend/monitoring/docs/QUICK_REFERENCE.md)** - Commands and queries cheat sheet
- **[Implementation Summary](./backend/monitoring/docs/IMPLEMENTATION_SUMMARY.md)** - Complete feature overview
- **[Main README](./backend/monitoring/README.md)** - Overview and architecture

#### Pre-configured Alerts

The system includes 7 alert rules:
- üö® High response time (>2s)
- üö® High error rate (>5%)
- üö® High CPU usage (>80%)
- üö® Critical memory usage (>95%)
- üö® Slow AI inference (>5s)
- üö® Service health failures
- üö® Request rate anomalies

#### Production Deployment

**Using Docker:**
```bash
cd backend/monitoring
docker-compose up -d
```

**For Choreo/Cloud:**
1. FastAPI automatically exposes `/metrics` endpoint
2. Deploy Prometheus to scrape metrics
3. Import Grafana dashboard (`grafana_dashboard.json`)
4. Configure Alertmanager for notifications (email/Slack)

#### Log Files

Logs are automatically created in `logs/` directory:
- `app.log` - All application logs
- `error.log` - Errors only
- `ai.log` - AI operations
- `ingestion.log` - GitHub ingestion

#### Useful Commands

```bash
# View real-time metrics
curl http://localhost:8000/metrics

# Check Prometheus targets
open http://localhost:9090/targets

# View live logs
tail -f logs/app.log

# Stop all monitoring services
cd backend/monitoring && ./stop.sh
```

### Built-in Monitoring Features

- **Health Checks**: `/api/health` endpoint with component status
- **Request Logging**: Automatic request/response logging with correlation IDs
- **Error Tracking**: Comprehensive error messages with stack traces
- **Performance Metrics**: Response time tracking via Prometheus
- **Distributed Tracing**: Ready for OpenTelemetry integration

### Additional Monitoring Options (Production)

- **Application Monitoring**: Azure Application Insights
- **Log Aggregation**: ELK Stack or Choreo Observability
- **Uptime Monitoring**: Pingdom or UptimeRobot
- **API Analytics**: Choreo API Management

---

## ü§ù Contributing

This is an internal project for WSO2 Choreo. For questions or issues:

1. Check existing documentation in `docs/readmes/`
2. Review [troubleshooting section](#-troubleshooting)
3. Contact the development team

---

## üìú License

Internal/example use for WSO2. Add your preferred license if publishing publicly.

---

## üôè Acknowledgments

- **WSO2 Choreo Team** - Platform and requirements
- **Azure OpenAI** - Language model capabilities
- **Milvus/Zilliz** - Vector database infrastructure
- **LangChain** - RAG framework and tools

---

## ‚öôÔ∏è Performance & Best Practices

### Conversation Memory Optimization

**Token Limits**
```python
# Recommended settings for different use cases

# Default (balanced)
max_total_tokens=8000
max_history_tokens=4000
summarization_trigger_ratio=0.75

# High-volume production (aggressive summarization)
max_total_tokens=6000
max_history_tokens=3000
summarization_trigger_ratio=0.6

# Development/testing (minimal summarization)
max_total_tokens=12000
max_history_tokens=8000
summarization_trigger_ratio=0.9
```

**Reducing Azure OpenAI Costs**
- Enable summarization to reduce token usage by 40-60%
- Set lower `max_history_tokens` for frequent, short conversations
- Use `enable_summarization: false` for single-turn questions
- Monitor token usage via `memory_stats` in responses

**Handling Peak Times**
```bash
# Temporarily disable LLM summarization during peak hours
export ENABLE_LLM_SUMMARIZATION=false

# Reduce retries to fail faster
export MAX_SUMMARIZATION_RETRIES=1
```

### Retrieval Optimization

**Quality vs Speed Trade-offs**
```python
# Current settings (balanced)
top_k=10  # Retrieve 10 candidates
score_threshold=0.7  # High quality filter

# For faster responses (lower quality)
top_k=5
score_threshold=0.6

# For best quality (slower)
top_k=15
score_threshold=0.75
```

**Query Enrichment**
- Conversation summary is limited to 300 characters for retrieval
- Only last 4 messages included in enriched query
- Balances context vs retrieval speed

### Streaming Performance

**First Token Time**
- Streaming: 1-2 seconds
- Non-streaming: 3-5 seconds
- Network overhead: ~200ms

**When to Use Streaming**
- ‚úÖ User-facing chat interfaces
- ‚úÖ Long responses (>500 tokens)
- ‚úÖ Better perceived performance
- ‚ùå Batch processing
- ‚ùå API integrations requiring full response

### Caching Strategies

**Frontend**
```javascript
// Conversations cached in localStorage
// Summary cached with each conversation
// No expiration (manual clear only)
```

**Backend**
```python
# No caching by default
# Consider adding:
# - Redis for conversation summaries
# - LRU cache for frequent queries
# - Milvus metadata cache
```

### Monitoring Performance

**Key Metrics to Watch**
```bash
# Visit http://localhost:8000/metrics

# Response time
http_request_duration_seconds

# Token usage
ai_tokens_total{type="input"}
ai_tokens_total{type="output"}

# Summarization
conversation_summary_created_total
conversation_summary_failed_total

# Vector search
vector_search_duration_seconds
```

**Performance Targets**
| Metric | Target | Warning | Critical |
|--------|--------|---------|----------|
| Response Time | <2s | >3s | >5s |
| First Token | <1.5s | >2s | >3s |
| Vector Search | <500ms | >1s | >2s |
| Summarization | <3s | >5s | >10s |
| Error Rate | <1% | >3% | >5% |

### Scaling Recommendations

**Small Deployments (<100 users)**
- Current configuration works well
- Single backend instance sufficient
- Local monitoring adequate

**Medium Deployments (100-1000 users)**
- Add Redis for conversation caching
- Multiple backend instances (3-5)
- Dedicated Prometheus/Grafana
- Consider Azure OpenAI Provisioned Throughput

**Large Deployments (1000+ users)**
- Kubernetes deployment
- Auto-scaling based on metrics
- Distributed caching (Redis Cluster)
- CDN for frontend assets
- Azure OpenAI Provisioned Throughput required
- Separate monitoring infrastructure

---

## ‚ùì Frequently Asked Questions

### Conversation Memory

**Q: Where is the conversation history stored?**
- A: In the frontend's localStorage for persistence across sessions. Each chat has its own history array.

**Q: Where is the summary stored?**
- A: The summary is returned in each API response and stored in the frontend alongside the conversation. It's updated automatically when needed.

**Q: When is a summary created?**
- A: Automatically when conversation history exceeds 75% of the token limit (default: 3,000 tokens out of 4,000 max).

**Q: Can I disable summarization?**
- A: Yes, set `ENABLE_LLM_SUMMARIZATION=false` in environment or `enable_summarization: false` in the request.

**Q: What happens if summarization fails (429 error)?**
- A: The system falls back to simple text-based summaries and continues working. See [Troubleshooting 429 Errors](./docs/readmes/TROUBLESHOOTING_429_ERRORS.md).

### Retrieval & Context

**Q: Does the system use conversation history for retrieval?**
- A: Yes! The query is enriched with conversation summary and recent messages before searching Milvus.

**Q: Are chunks from Milvus sent to the LLM?**
- A: Yes, the top 5-10 high-quality chunks are included in the LLM context along with the conversation.

**Q: How does the filtering work?**
- A: OpenChoreo content is filtered at multiple stages: during retrieval, before sending to LLM, and when displaying sources.

### Streaming

**Q: Is streaming enabled by default?**
- A: Yes, the frontend automatically uses the streaming endpoint (`/api/ask/stream`).

**Q: What if streaming fails?**
- A: The frontend automatically falls back to the standard `/api/ask` endpoint.

**Q: Can I use streaming via API?**
- A: Yes, use `POST /api/ask/stream?question=Your+question` with curl's `-N` flag.

### Performance

**Q: Does conversation memory slow down responses?**
- A: No, it actually improves speed by reducing tokens sent to the LLM. Summarization only happens when needed.

**Q: How many conversations can I have?**
- A: Unlimited! Each conversation is stored separately in localStorage.

**Q: What's the maximum conversation length?**
- A: Practically unlimited due to automatic summarization. Very long conversations are compressed efficiently.

### Private Repository Information

**Q: Why does the assistant refuse to share internal/private details?**
- A: This was a previous configuration. The system now shares ALL information from the knowledge base, including private repositories, as it's designed for internal WSO2 Choreo developers.

**Q: Can I see internal API endpoints like Rudder?**
- A: Yes! If this information is in the ingested repositories, DevChoreo will share it. The system is configured to provide complete technical details.

**Q: How do I ensure private repo data is included?**
- A: Make sure private repositories are ingested using the `/api/ingest/github` endpoint with proper GitHub token authentication.

---

## üìû Support & Resources

- **Documentation**: [docs/readmes/INDEX.md](./docs/readmes/INDEX.md)
- **API Docs**: http://localhost:8000/docs
- **Choreo Platform**: https://console.choreo.dev/
- **Security Audit**: [SECURITY_AUDIT_REPORT.md](docs/implementation/SECURITY_AUDIT_REPORT.md)

---

**Last Updated:** December 2, 2025  
**Version:** 2.0.0  
**Status:** Production Ready ‚úÖ

## üìã Recent Updates (v2.0.0)

### November-December 2025

**üß† Conversation Memory System**
- ‚úÖ Intelligent LLM-powered summarization
- ‚úÖ Token tracking and management
- ‚úÖ Metadata extraction (topics, questions, decisions)
- ‚úÖ Graceful fallback for peak times
- ‚úÖ Per-request configuration options

**‚ö° Progressive Streaming**
- ‚úÖ ChatGPT-like word-by-word responses
- ‚úÖ Streaming cursor indicator
- ‚úÖ Server-Sent Events (SSE) implementation
- ‚úÖ Automatic fallback to standard API

**üîç Enhanced Retrieval**
- ‚úÖ Context-aware query enrichment
- ‚úÖ Conversation history integration
- ‚úÖ Quality-based filtering
- ‚úÖ OpenChoreo content exclusion

**üõ°Ô∏è Reliability Improvements**
- ‚úÖ Azure OpenAI 429 error handling
- ‚úÖ Retry logic with exponential backoff
- ‚úÖ Environment-based feature toggles
- ‚úÖ Comprehensive error logging

**üìä Monitoring & Observability**
- ‚úÖ 23+ Prometheus metrics
- ‚úÖ Pre-built Grafana dashboard
- ‚úÖ Smart alerting rules
- ‚úÖ Structured JSON logging

**üìö Documentation**
- ‚úÖ Comprehensive feature guides
- ‚úÖ Visual diagrams and flowcharts
- ‚úÖ Troubleshooting guides
- ‚úÖ Quick start examples

---

## Quick Start

For detailed instructions, see:
- **[Setup Guide](./docs/readmes/SETUP_GUIDE.md)** - Complete setup instructions
- **[Run Project](./docs/readmes/RUN_PROJECT.md)** - How to run the application
- **[Docker Guide](./docs/readmes/DOCKER_README.md)** - Docker deployment
- **[Incremental Ingestion](./docs/readmes/INCREMENTAL_INGESTION.md)** - Smart chunking feature

## Prerequisites
- Python 3.12+
- Node.js 18+ and npm
- Accounts/keys for:
  - Azure OpenAI (chat + embeddings deployments)
  - Milvus (vector database instance)

---

## 1) Backend setup

1. Export environment variables (or create a `backend/.env`). Example:

```bash
# Azure OpenAI
export AZURE_OPENAI_KEY="your_azure_openai_key"
export AZURE_OPENAI_ENDPOINT="https://your-azure-openai-endpoint.openai.azure.com/"
export AZURE_OPENAI_DEPLOYMENT="your_chat_deployment_name"
# Optional: separate embeddings deployment (recommended)
export AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT="your_embeddings_deployment_name"
# Optional: API version (the code has a default)
export AZURE_OPENAI_API_VERSION="2024-02-15-preview"

# Milvus
export MILVUS_HOST="localhost"
export MILVUS_PORT="19530"
export MILVUS_COLLECTION_NAME="choreo_docs"
export MILVUS_USER=""
export MILVUS_PASSWORD=""
export MILVUS_DB_NAME="default"
```

2. Install Python dependencies and run the API:

```bash
cd "choreo-ai-assistant"
python -m pip install --upgrade pip
python -m pip install -r choreo-ai-assistant/requirements.txt
uvicorn backend.app:app --host 0.0.0.0 --port 8000
```

3. Health check:

- Visit http://localhost:8000/health
- If keys are not valid or network is restricted, health may show "unhealthy"; the server can still start.

### Ingest the GitHub repo (wso2/docs-choreo-dev)
Run once to populate Milvus:

```bash
curl -X POST "http://localhost:8000/ingest/github" \
  -H "Content-Type: application/json" \
  -d '{"repo_url":"https://github.com/wso2/docs-choreo-dev.git","branch":"main"}'
```

### GitHub Webhook (optional, to auto-update on push)
- In your GitHub repo: Settings ‚Üí Webhooks ‚Üí Add webhook
  - Payload URL: `http://YOUR_HOST:8000/webhook/github`
  - Content type: `application/json`
  - Events: `Just the push event`
  - Secret: optional (current endpoint does not verify signatures)

---

## 2) Frontend setup

1. Install and start the dev server:

```bash
cd "choreo-ai-assistant/frontend"
npm install
npm run dev
```

2. Open http://localhost:5173

- The dev server proxies `/api` to `http://localhost:8000` (see `frontend/vite.config.js`).
- UI features: New chat, list/switch chats, rename, delete, persistent chat history via localStorage.

---

## Docker (optional: backend only)

```bash
cd "choreo-ai-assistant/docker"
# Ensure env vars are exported in your shell before this step
# (same variables as above for Azure OpenAI + Milvus)
docker compose up --build
```

Backend will be available on http://localhost:8000.

---

## API quick reference
- `GET /health` ‚Äî Health check (Milvus connectivity)
- `POST /ask?question=...` ‚Äî Ask a question (RAG using similarity from Milvus)
- `POST /ask_graph?question=...` ‚Äî Ask via LangGraph pipeline
- `POST /ingest/github` ‚Äî Body: `{ "repo_url": "...", "branch": "main" }`
- `POST /webhook/github` ‚Äî Basic push webhook; re-ingests repo from payload

---

## Troubleshooting
- Azure OpenAI errors: verify endpoint URL, API key, and deployment names; set `AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT` if using a separate embeddings model.
- Milvus connection errors: verify Milvus host, port, and credentials are correct; ensure Milvus is running and accessible.
- Frontend API errors: confirm the backend is running on port 8000 and the Vite proxy is active (run `npm run dev`).

---

## License
Internal/example use. Add your preferred license if publishing.
